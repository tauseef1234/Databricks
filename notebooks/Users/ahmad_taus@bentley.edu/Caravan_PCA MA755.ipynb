{"cells":[{"cell_type":"markdown","source":["#Caravan PCA \nDate: 02/26/2018 <br \\>"],"metadata":{}},{"cell_type":"markdown","source":["## Introduction"],"metadata":{}},{"cell_type":"markdown","source":["The objective of this notebook is to investigate the correlation between variables after performing Principal Component Analysis. We start by defining a class called MyPCA that does the same thing as the sklearn PCA. After testing the performance of this class, we apply it in a pipeline to the caravan train data. The analyses proceed by adding two different estimators(Logistic Regression and KNN) to the pipeline. Finally, we evaluate the train missclassification rate of each of these estimators with cross validation.\n\nFor the prediction task, the underlying problem is to the find the subset of customers with a probability of having a caravan insurance policy above some boundary probability. The known policyholders can then be removed and the rest receives a mailing. The boundary depends on the costs and benefits such as of the costs of mailing and benefit of selling insurance policies."],"metadata":{}},{"cell_type":"markdown","source":["## 1. Dataset Description"],"metadata":{}},{"cell_type":"markdown","source":["The original dataset consists of 87 attributes and 9822 observations. The dataset used in this notebook has a reduced set of variables containing only the 12 predictor variables selected (after using corrleation and random fores) including the response variable `CARAVAN`. The number of the observations remains the same."],"metadata":{}},{"cell_type":"markdown","source":["## 2. Load Libraries"],"metadata":{}},{"cell_type":"markdown","source":["Load the required libraries and check version numbers."],"metadata":{}},{"cell_type":"code","source":["import sklearn as sk\nimport numpy             as np\nimport pandas            as pd\nimport matplotlib        as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.__version__, pd.__version__, mpl.__version__, sns.__version__"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Load the DataFrameMapper function and others that are needed from sklearn."],"metadata":{}},{"cell_type":"code","source":["%sh /databricks/python3/bin/pip3 install plotly"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from sklearn_pandas import DataFrameMapper\nfrom sklearn_pandas import gen_features\nfrom sklearn import pipeline\nimport sklearn.preprocessing, sklearn.decomposition, \\\n       sklearn.linear_model,  sklearn.pipeline, \\\n       sklearn.metrics\nfrom sklearn.decomposition import PCA\nimport scipy"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## 3. Define Functions"],"metadata":{}},{"cell_type":"markdown","source":["Define `MyPCA` class that replaces the `PCA` class from Scikit-learn in the following ways:\n- store the eigen-values sorted from highest to lowest in an attribute named `explained_variance_`\n- store the eigen-vectors as row vectors in an attibute named `components_`\n- the eigen-vectors should be in the same as order as the corresponding eigen-values\n- create a `transform` method performs identically to that method in the `PCA` class\n- create a `fit` method that does nothing except `return self`\n\nWe assume that the input matrix has zero column means."],"metadata":{}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin\n\nclass MyPCA(BaseEstimator, TransformerMixin):\n  def __init__(self, n_components=False):\n    self.n_components = n_components   \n  def fit(self, X, y=None):\n    return self\n  def transform(self, X):\n    covariance_matrix = X.T.dot(X)/len(X) # calculate the covariance matrix\n    self.explained_variance_, self.components_= np.linalg.eig(covariance_matrix) # get the eigen values and the eigen vectors\n    Q = self.components_  # assign the eigen vectors to variable Q \n    X=np.round(X.dot(Q),decimals=2) # get the dot product of X and Q\n    idx =self.explained_variance_.argsort()[::-1]  # get the indices of eigen values sorted, the argsort() sorts them in ascending order, [::-1] gives the reverse of it which is the descending order\n    self.explained_variance_ = self.explained_variance_[idx] # sort the eigen values\n    self.components_ = self.components_[:,idx].T  # get the transpose of the eigen vectors so that they are displayed in rows not columns\n    return X"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## 4. Read Dataset"],"metadata":{}},{"cell_type":"markdown","source":["Read the dataset and display the first three observations."],"metadata":{}},{"cell_type":"code","source":["caravan_df = pd.read_csv('/dbfs/mnt/group-ma755/data/caravan-insurance-challenge.csv')\ncaravan_df.head(3)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Reduce the dimensions of the data to contain only the 12 variables."],"metadata":{}},{"cell_type":"code","source":["most_imp_var= ['APERSAUT', 'MINKGEM', 'MKOOPKLA','MINKM30','AWAPART', 'MHHUUR', 'MOPLLAAG', 'ALEVEN', 'APLEZIER', 'ABRAND', 'MOSHOOFD', 'CARAVAN']"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["red_caravan_df = caravan_df[most_imp_var]"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Check the structure of the dataset. The dataset contains 12 variables including the target variable."],"metadata":{}},{"cell_type":"code","source":["red_caravan_df.info()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## 5. Prepare the Dataset for Analyses"],"metadata":{}},{"cell_type":"markdown","source":["Convert the data types to float."],"metadata":{}},{"cell_type":"code","source":["red_caravan_df = red_caravan_df.astype(float)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Check the dataset."],"metadata":{}},{"cell_type":"code","source":["red_caravan_df.info()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Create the train and the test data."],"metadata":{}},{"cell_type":"code","source":["red_caravan_df_train= red_caravan_df.loc[caravan_df['ORIGIN']=='train',:]\nred_caravan_df_test = red_caravan_df.loc[caravan_df['ORIGIN']=='test']"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["red_caravan_df_train.shape"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Specify the train and test for the target variable."],"metadata":{}},{"cell_type":"code","source":["y_train = red_caravan_df_train['CARAVAN'].astype('category')\ny_test =red_caravan_df_test['CARAVAN'].astype('category')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Specify the train and test data for the attributes."],"metadata":{}},{"cell_type":"code","source":["x_train=red_caravan_df_train.iloc[:,0:11]\nx_test=red_caravan_df_train.iloc[:,11]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Define the binarizer list."],"metadata":{}},{"cell_type":"code","source":["binarizer_list = [['MOSHOOFD']]"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Create a list of lists of variables to be scaled and store it in `scaler_list`."],"metadata":{}},{"cell_type":"code","source":["scaler_list = [[name] for name in list(red_caravan_df.columns) if name not in ['CARAVAN', 'MOSHOOFD'] ]\nscaler_list"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## 6. DataFrameMapper and FeatureUnion"],"metadata":{}},{"cell_type":"markdown","source":["#### 6.1 DataFrameMapper\n\n- DataFrameMapper functionality maps the original pandas dataframe columns into transformations and then returns a tuple that can be used with the ML algorithm."],"metadata":{}},{"cell_type":"markdown","source":["Create a binarizer mapper for the categorical variables. The `LabelBinarizer` is applied to each of the column in the `binarizer_list`."],"metadata":{}},{"cell_type":"code","source":["binarizer_mapper = DataFrameMapper(gen_features(columns=binarizer_list,\n                                                classes=[{'class': sklearn.preprocessing.LabelBinarizer}]))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Create a `scaler_mapper` for the numerical variables. The `StandardScaler` is applied to each of the columns in the `scaler_list`."],"metadata":{}},{"cell_type":"code","source":["scaler_mapper = DataFrameMapper(gen_features(columns=scaler_list,\n                                                classes=[{'class': sklearn.preprocessing.StandardScaler}]))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["__What does scaler mapper exactly do?__\n\nScaler mapper standardizes features by setting the mean to zero and scaling to unit variance. Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using the transform method.\n\n__Why do we need to standardize/normalize?__\n\nNormalization is an important preprocessing step before applying Principle Component Analysis (PCA). In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\n\n_http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\nhttp://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py"],"metadata":{}},{"cell_type":"markdown","source":["#### 6.2 FeatureUnion\n\n- Concatenates results of multiple transformer objects. This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer."],"metadata":{}},{"cell_type":"markdown","source":["Define a `feature_union` that concatenates the results of the binarizer and the scaler mapper defined above."],"metadata":{}},{"cell_type":"code","source":["feature_union = sklearn.pipeline.FeatureUnion([('binarizer', binarizer_mapper), \n                                      ('scaler',    scaler_mapper),])"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["df=pd.DataFrame(feature_union.fit(x_train).transform(x_train))"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## 7. PCA in Pipeline"],"metadata":{}},{"cell_type":"markdown","source":["### 7.1 PCA with custom class: MyPCA"],"metadata":{}},{"cell_type":"markdown","source":["Define a pipeline that contains the feature union and `MyPCA` class."],"metadata":{}},{"cell_type":"code","source":["pipeline_1 = pipeline.Pipeline([('feature_union', feature_union),\n                                  ('pca', MyPCA())])\n\npipeline_1"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["Fit transform the train data in `Pipeline_1`."],"metadata":{}},{"cell_type":"code","source":["pca_with_class = pipeline_1.fit_transform(x_train)\npca_with_class"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["Check the shape of the pipeline output."],"metadata":{}},{"cell_type":"code","source":["pca_with_class.shape"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Display the explained variance/ eigen values of the MyPCA class result."],"metadata":{}},{"cell_type":"code","source":["eigen_values = pipeline_1.named_steps['pca'].explained_variance_\neigen_values"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["Display the components/eigen vectors of the MyPCA class result."],"metadata":{}},{"cell_type":"code","source":["eigen_vectors = pipeline_1.named_steps['pca'].components_\neigen_vectors"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["### 7.2 PCA with `sklearn.decomposition.PCA`"],"metadata":{}},{"cell_type":"markdown","source":["Define a pipeline that contains the feature union and `PCA` class from sklearn library. Fit transform the train data in `Pipeline_2`."],"metadata":{}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\npipeline_2 = pipeline.Pipeline([('feature_union', feature_union),\n                                ('sklearn_pca', sklearn.decomposition.PCA(20))])\n"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["__What does pipeline do?__\n\nSequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument."],"metadata":{}},{"cell_type":"markdown","source":["Fit transform the train data in `Pipeline_2`."],"metadata":{}},{"cell_type":"code","source":["pca_2 =np.round(pipeline_2.fit_transform(x_train), decimals=2)\npca_2"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["Display the explained variance/ eigen values of the `PCA` result."],"metadata":{}},{"cell_type":"code","source":["eigen_values_2 =pipeline_2.named_steps['sklearn_pca'].explained_variance_\neigen_values_2"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["eigen_values.shape"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["Display the components/eigen vectors of the `PCA` result."],"metadata":{}},{"cell_type":"code","source":["pca_comp = pipeline_2.named_steps['sklearn_pca'].components_"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["pca_df=pd.DataFrame(pca_comp)\npca_df"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["components=pd.DataFrame(pca_comp, columns = df.columns)\ncomponents[:1]"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["eigen_vectors_2 =pipeline_2.named_steps['sklearn_pca'].components_\neigen_vectors_2"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["pipeline_2.named_steps['sklearn_pca'].explained_variance_ratio_"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["### 7.3 Comparison of the results"],"metadata":{}},{"cell_type":"markdown","source":["Check the first row of the two `pca` outputs. The output of the `pca` is the dot product of the feature union output and the eigen vectors."],"metadata":{}},{"cell_type":"code","source":["pca_with_class.shape"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly.graph_objs import *\nimport plotly.tools as tls\nfrom plotly.offline import plot"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["tot = sum(eigen_values_2)\nvar_exp = [(i / tot)*100 for i in eigen_values_2]\ncum_var_exp = np.cumsum(var_exp)\n\ntrace1 = Bar(\n        x=['PC %s' %i for i in range(1,5)],\n        y=var_exp,\n        showlegend=False)\n\ntrace2 = Scatter(\n        x=['PC %s' %i for i in range(1,5)], \n        y=cum_var_exp,\n        name='cumulative explained variance')\n\ndata = Data([trace1, trace2])\n\nlayout=Layout(xaxis=XAxis(title='Principal Components'),\n        yaxis=YAxis(title='Explained variance in percent'),\n        title='Explained variance by different principal components using `sklearn.PCA`')\n\nfig = Figure(data=data, layout=layout)\n\nt=plot(fig,output_type='div')\n"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["displayHTML(t)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["tot = sum(eigen_values)\nvar_exp = [(i / tot)*100 for i in eigen_values]\ncum_var_exp = np.cumsum(var_exp)\n\ntrace1 = Bar(\n        x=['PC %s' %i for i in range(1,5)],\n        y=var_exp,\n        showlegend=False)\n\ntrace2 = Scatter(\n        x=['PC %s' %i for i in range(1,5)], \n        y=cum_var_exp,\n        name='cumulative explained variance')\n\ndata = Data([trace1, trace2])\n\nlayout=Layout(xaxis=XAxis(title='Principal Components'),\n        yaxis=YAxis(title='Explained variance in percent'),\n        title='Explained variance by different principal components using `MyClassPCA`')\n\nfig = Figure(data=data, layout=layout)\n\nt1=plot(fig,output_type='div')"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["displayHTML(t1)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["p = plot(\n  [\n    Histogram2dContour(x=pca_2[0], y=pca_2[1], contours=Contours(coloring='heatmap')),\n    Scatter(x=pca_2[0], y=pca_2[1], mode='markers', marker=Marker(color='white', size=3, opacity=0.3))\n  ],\n  output_type='div'\n)\n\ndisplayHTML(p)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["pca_2[0]"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["len(pca_2[0].transpose())"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["import plotly.graph_objs as go"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["trace1 = go.Scatter3d(\n    x=pca_2[0][:20],\n    y=pca_2[0][:20],\n    z=pca_2[2][:20],\n    mode='markers',\n    marker=dict(\n        size=12,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=0.8\n    )\n)\n\n\ntrace2 = go.Scatter3d(\n    x=pca_2[0].transpose(),\n    y=pca_2[1].transpose(),\n    z=pca_2[2].transpose(),\n    mode='markers',\n    marker=dict(\n        color='rgb(127, 127, 127)',\n        size=12,\n        symbol='circle',\n        line=dict(\n            color='rgb(204, 204, 204)',\n            width=1\n        ),\n        opacity=0.9\n    )\n)\ndata = [trace1, trace2]\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=data, layout=layout)\nu=plot(fig,output_type='div')"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["displayHTML(u)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["pca_with_class[2], pca_2[2]"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["Comparing the eigen values of the two `pca` outputs."],"metadata":{}},{"cell_type":"code","source":["eigen_values[0:5], eigen_values_2[0:5]"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["Comparing the eigen vectors of the two `pca` outputs."],"metadata":{}},{"cell_type":"code","source":["eigen_vectors[0], eigen_vectors_2[0]"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["### 7.4 Investigation of the Principal Components of `PCA`"],"metadata":{}},{"cell_type":"markdown","source":["The output of the pca is the dot product of A and Q. Where A is the mean centered data and Q is the eigen vectors."],"metadata":{}},{"cell_type":"code","source":["pca_with_class"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":["Display the covariance matrix of MyPCA class output."],"metadata":{}},{"cell_type":"code","source":["cov = np.cov(m=pca_with_class, rowvar=False, bias=True)\nprint(np.round(cov, decimals=2)[0:10,0:10])"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"markdown","source":["This is a diagonal matrix which has non zeros on the diagonals. These values are non zero because they are the variances of the new variables. The rest of the values are all zero, which means that all of the variables are now uncorrelated."],"metadata":{}},{"cell_type":"markdown","source":["## 8. Estimators"],"metadata":{}},{"cell_type":"markdown","source":["### 8.1 Logistic Regression with Sklearn PCA"],"metadata":{}},{"cell_type":"markdown","source":["Create an estimator pipeline with feature union, sklearn pca and logistic regression estimator."],"metadata":{}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\npipeline_3 = pipeline.Pipeline([('feature_union', feature_union),\n                                  ('pca', sklearn.decomposition.PCA(20)),\n                                ('logistic', sklearn.linear_model.LogisticRegression())])\n   \npipeline_3"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"markdown","source":["Perform cross validation to get the accurate train MSE."],"metadata":{}},{"cell_type":"code","source":["# evaluate the pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nseed = 7\nkfold = KFold(n_splits=15, random_state=seed)\nresults = cross_val_score(pipeline_3, x_train, y_train, cv=kfold)\nprint(results)"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"markdown","source":["The training set is split into 15 different kfolds. In the first round, the model is trained in the last 14 folds and the first fold is kept aside also called as validation set, which is used to calculate the misclassification rate. The second round saves the second fold to calculate the error rate and trains the model in the rest of the folds. The third round saves the third fold to calculate the error rate and trains the model in the rest of the folds. The same logic applies in the other rounds too. The results of the error rate is displayed above. We calculate the mean of these values to get the accurate train error rate."],"metadata":{}},{"cell_type":"markdown","source":["Print the mean of the 15 error rates."],"metadata":{}},{"cell_type":"code","source":["print(results.mean())"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":["### 8.2 Logistic Regression with MyPCA Class"],"metadata":{}},{"cell_type":"markdown","source":["In this subsection we perform the same analysis as above, but using MyPCA class. The train misclassification rate is the same until the third decimal place."],"metadata":{}},{"cell_type":"markdown","source":["Create an estimator pipeline with feature union, MyPCA class and logistic regression estimator."],"metadata":{}},{"cell_type":"code","source":["pipeline_4 = pipeline.Pipeline([('feature_union', feature_union),\n                                  ('pca', MyPCA()),\n                                ('logistic', sklearn.linear_model.LogisticRegression())])\n   \npipeline_4"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":["Perform cross validation to get the accurate train misclassfication rate."],"metadata":{}},{"cell_type":"code","source":["kfold = KFold(n_splits=15, random_state=seed)\nresults = cross_val_score(pipeline_4, x_train, y_train, cv=kfold)\nprint(results)"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"markdown","source":["The training set is split into 15 different kfolds. In the first round, the model is trained in the last 14 folds and the first fold is kept aside also called as validation set, which is used to calculate the misclassification rate. The second round saves the second fold to calculate the error rate and trains the model in the rest of the folds. The third round saves the third fold to calculate the error rate and trains the model in the rest of the folds. The same logic applies in the other rounds too. The results of the error rates is displayed above. We calculate the mean of these values to get the accurate train error rate."],"metadata":{}},{"cell_type":"markdown","source":["Print the mean of the 15 error rate's."],"metadata":{}},{"cell_type":"code","source":["print(results.mean())"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"markdown","source":["### 8.3 KNN with MyPCA"],"metadata":{}},{"cell_type":"markdown","source":["Create an estimator pipeline with feature union, sklearn pca and KNN estimator. KNN estimates the conditional distribution of Y given X, and then classifies a\ngiven observation to the class with highest estimated probability."],"metadata":{}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\npipeline_5 = pipeline.Pipeline([('feature_union', feature_union),\n                                  ('pca', MyPCA()),\n                                ('knn', KNeighborsClassifier(n_neighbors=5))])\n   \npipeline_5"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"markdown","source":["%md Perform cross validation to get the accurate train MSE."],"metadata":{}},{"cell_type":"code","source":["scores =cross_val_score(pipeline_5, x_train, y_train, cv=kfold, scoring='accuracy')\nprint(scores)"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"markdown","source":["The training set is split into 15 different kfolds. In the first round, the model is trained in the last 14 folds and the first fold is kept aside also called as validation set, which is used to calculate the missclassification rate. The second round saves the second fold to calculate the misclassfication rate and trains the model in the rest of the folds. The third round saves the third fold to calculate the error rate and trains the model in the rest of the folds. The same logic applies in the other rounds too. The results of the  misclassification rate is displayed above. We calculate the mean of these values to get the accurate train error rate."],"metadata":{}},{"cell_type":"markdown","source":["Display the mean of the 15 MSE's."],"metadata":{}},{"cell_type":"code","source":["print(scores.mean())"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"markdown","source":["### 8.4 Comparision of Logistic and KNN"],"metadata":{}},{"cell_type":"markdown","source":["Visualize the change in  misclassification rate for logistic cross validation (in blue) and KNN cross validation (in orange). The error rate is higher for logistic regression."],"metadata":{}},{"cell_type":"code","source":["plt.gcf().clear()\nplt.plot(results)\nplt.plot(scores)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"markdown","source":["The visualization shows the changing error rate for different folds of the data. If we do not perform cross validation and get the mean, the train error rate value would not have been very accurate. The train error rate of the two methods is very close. KNN's train is 0.938 while logistic misclassification rate is 0.934. Either one of the models can be used to make predictions."],"metadata":{}},{"cell_type":"markdown","source":["## Appendix: Test MyPCA Class with small simple data"],"metadata":{}},{"cell_type":"markdown","source":["Create a numpy matrix, called X, which is our input to the MyPCA class."],"metadata":{}},{"cell_type":"code","source":["X_nrows = 5\nX_ncols = 3\nimport random\nrandom.seed(1)\nX = np.random.randint(9,size=(X_nrows,X_ncols))\nX"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"markdown","source":["Create matrix A by subtracting the mean of each column (from each row.)"],"metadata":{}},{"cell_type":"code","source":["A = (X - np.mean(X,axis=0))\nA"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"markdown","source":["Apply the sklearn PCA  `fit_transform` method to matrix A."],"metadata":{}},{"cell_type":"code","source":["trial = PCA(n_components=X_ncols)\ntrial.fit_transform(A)\ntrial.components_"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"markdown","source":["Apply the MyPCA class `fit_transform` method to matrix A."],"metadata":{}},{"cell_type":"code","source":["trial2= MyPCA()\ntrial2.fit_transform(A)\ntrial2.components_"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"markdown","source":["Display the eigen values from both the sklearn PCA and MyPCA class"],"metadata":{}},{"cell_type":"code","source":["trial.explained_variance_"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"code","source":["trial2.explained_variance_"],"metadata":{},"outputs":[],"execution_count":143},{"cell_type":"code","source":["np.array_equal(trial.explained_variance_, trial2.explained_variance_) "],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"markdown","source":["Conclusion on PCA class Test: Class gives the same results as PCA sklearn just the components have flipped signs. Eigen values are exactly the same. We did research on this topic and it seems to be not be an issue. For reference please refer to the links below:\n- https://stats.stackexchange.com/questions/30348/is-it-acceptable-to-reverse-a-sign-of-a-principal-component-score \n- https://www.reddit.com/r/statistics/comments/5kz8fo/why_do_the_signs_flip_when_getting_normalized_pca/"],"metadata":{}},{"cell_type":"markdown","source":["### Conclusion"],"metadata":{}},{"cell_type":"markdown","source":["The first two pipelines generated in this notebook contain a feature union and principal component analysis. The first pipeline has MyPCA class while the other one has the PCA from Sklearn. Feature union contains two different dataframe mappers; a binarizer mapper for the categorical data and a scaler mapper for the numerical ones. The scaler mapper centers the data to have mean zero and standard deviation one. Both of the analyses gave the same values of eigen values and eigen vectors. One of the interesting facts that we discovered is that MyPCA class output and the eigen vectors have flipped signs compared to the Sklearn PCA. Part of our research was investigating the reason of this situation, which led us to the conclusion that this is not an issue and will not impact our analyses. \n\nThe pca output, which is the dot product of feature union output and the eigen vectors, was used to calculate the covariance matrix. The covariance matrix had non-zero values for diagonal places and zero values for all the other ones. This means that there is no correlation between the variables. The diagonal non-zero values correspond to the covariance of one variable with itself, which in this case is the variance. \n\nAfter achieving non correlated variables we then trained a logistic regression model and a KNN model. The cross validation is performed to achieve an accurate train error rate of the models. Looking at the mean error rate we can conclude that the two models performed almost the same. Any of the two can be used to predict on the test data."],"metadata":{}}],"metadata":{"name":"Caravan_PCA MA755","notebookId":170410},"nbformat":4,"nbformat_minor":0}